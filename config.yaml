# 预训练数据文件路径
file_path: 'data/pretrain.csv'          # pretrain data path
# 总的训练轮次
epochs: 30                              # total number of epochs
# 批量大小
batch_size: 100                         # batch size
# 学习率
lr_rate: 0.00005                        # learning rate
# 学习率调度器类型
scheduler_type: 'linear'                # scheduler type
# AdamW优化器的权重衰减系数，
#作用是在loss函数中加入一个惩罚项,使得模型权重向0收缩,从而避免过拟合。
weight_decay: 0.0                       # weight decay for AdamW
# 学习率预热比例
warmup_ratio: 0.05                      # warm-up ratio for scheduler
# 模型保存策略(每轮保存/每次验证保存)
save_strategy: 'epoch'                  # save strategy of trainer
# 是否覆盖输出目录
overwrite_output_dir: True              # whether to overwrite output directory (i.e. True/False)
# 最多保留的模型checkpoints数量
save_total_limit: 3                    # save total limit of trainer
# 是否使用 FP16 混合精度训练
fp16: True                              # float precision 16 (i.e. True/False)
# 日志记录频率
logging_strategy: 'epoch'               # logging frequency
# 验证频率
evaluation_strategy: 'epoch'            # validation frequency
# 模型训练/验证结果输出方式(TensorBoard)
report_to: 'tensorboard'                # integrations to report the results and logs to
# 数据加载器使用的子进程数量
dataloader_num_workers: 18              # Number of subprocesses to use for data loading
# 是否开启 Sharded DDP 训练
sharded_ddp: False                   # option of Sharded DDP training
# 预训练模型的保存路径
save_path: 'ckpt/pretrain.pt'           # logging and save path of the pretrained model
# 是否载入预训练模型checkpoint
load_checkpoint: True

# Transformer编码器的最大位置编码长度
max_position_embeddings: 514            # max position embeddings of Transformer
# 最大序列长度 (tokenization之后)
blocksize: 175                          # max length of sequences after tokenization
# Transformer编码器的每个隐藏层中的注意力头数
num_attention_heads: 12                 # number of attention heads in each hidden layer
# Transformer编码器的隐藏层数
num_hidden_layers: 6                    # number of hidden layers
# 隐藏层dropout比率
hidden_dropout_prob: 0.1                # hidden layer dropout
# 注意力dropout比率 
attention_probs_dropout_prob: 0.1       # attention dropout
# 掩码语言模型的掩码概率
mlm_probability: 0.15                   # masked probability in mlm

